# src/debug_transformer.py
import torch
import sys
import os

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.transformer import Transformer, MultiHeadAttention
from src.config import Config


def debug_attention():
    """è°ƒè¯•å¤šå¤´æ³¨æ„åŠ›"""
    print("è°ƒè¯•å¤šå¤´æ³¨æ„åŠ›...")

    config = Config()

    # æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›
    try:
        attention = MultiHeadAttention(d_model=64, nhead=4).to(config.device)

        # æµ‹è¯•è‡ªæ³¨æ„åŠ›
        batch_size = 2
        seq_len = 10
        x = torch.randn(batch_size, seq_len, 64).to(config.device)

        output = attention(x, x, x)
        print(f"âœ… è‡ªæ³¨æ„åŠ›æˆåŠŸ! è¾“å…¥å½¢çŠ¶: {x.shape}, è¾“å‡ºå½¢çŠ¶: {output.shape}")

        # æµ‹è¯•äº¤å‰æ³¨æ„åŠ›ï¼ˆä¸åŒåºåˆ—é•¿åº¦ï¼‰
        query_len = 8
        key_len = 12
        query = torch.randn(batch_size, query_len, 64).to(config.device)
        key = torch.randn(batch_size, key_len, 64).to(config.device)
        value = torch.randn(batch_size, key_len, 64).to(config.device)

        output = attention(query, key, value)
        print(f"âœ… äº¤å‰æ³¨æ„åŠ›æˆåŠŸ! query: {query.shape}, key: {key.shape}, value: {value.shape}, è¾“å‡º: {output.shape}")

        return True

    except Exception as e:
        print(f"âŒ æ³¨æ„åŠ›è°ƒè¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def debug_transformer():
    """è°ƒè¯•å®Œæ•´Transformer"""
    print("è°ƒè¯•å®Œæ•´Transformer...")

    config = Config()

    try:
        model = Transformer(
            src_vocab_size=1000,
            tgt_vocab_size=1000,
            d_model=config.d_model,
            nhead=config.nhead,
            num_encoder_layers=config.num_encoder_layers,
            num_decoder_layers=config.num_decoder_layers,
            dim_feedforward=config.dim_feedforward,
            dropout=config.dropout
        ).to(config.device)

        print("âœ… Transformeråˆ›å»ºæˆåŠŸ!")
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

        # æµ‹è¯•ä¸åŒåºåˆ—é•¿åº¦çš„è¾“å…¥
        batch_size = 2
        src_len = 10
        tgt_len = 8

        src = torch.randint(0, 1000, (batch_size, src_len)).to(config.device)
        tgt = torch.randint(0, 1000, (batch_size, tgt_len)).to(config.device)

        # ç®€å•æ©ç 
        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)
        tgt_mask = torch.tril(torch.ones(tgt_len, tgt_len, device=config.device))
        tgt_mask = tgt_mask.unsqueeze(0).unsqueeze(1)

        output = model(src, tgt[:, :-1], src_mask, tgt_mask[:, :, :-1, :-1])

        print("âœ… Transformerå‰å‘ä¼ æ’­æˆåŠŸ!")
        print(f"è¾“å…¥å½¢çŠ¶: src {src.shape}, tgt {tgt.shape}")
        print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")

        return True

    except Exception as e:
        print(f"âŒ Transformerè°ƒè¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    print("=" * 50)
    print("Transformerè°ƒè¯•")
    print("=" * 50)

    success1 = debug_attention()
    success2 = debug_transformer()

    if success1 and success2:
        print("ğŸ‰ æ‰€æœ‰è°ƒè¯•é€šè¿‡!")
    else:
        print("âŒ è°ƒè¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ")